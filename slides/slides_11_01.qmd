---
title: "Social Statistics"
subtitle: "OLS With Multiple Variables"
date: "November 28, 2023"
format: 
  clean-revealjs:
    incremental: true
    progress: false
    slide-number: false
    controls: auto
    menu: 
      side: left
---

```{r, include = FALSE, warning = FALSE}
library(tidyverse)
library(kableExtra)
gss_week11 <- read_csv("../data/gss_week_11.csv")
```


## Warm Up In Groups

- Regress age at first birth (`agekdbrn`) on years of education (`educ`)
  - Predict age at first birth for respondents with 16 years of education

- Regress age at first birth (`agekdbrn`) on degree (`degree`)
  - Use "College Degree" as the reference group. Predict age at first birth for respondents with a grad/prof degree

- Regress having a first child at age 30 or later (`agekdbrn_30plus`) on religion (`religion`)
  - Use "Protestant" as the reference group. Predict probability of having a first child at age 30 or later for Jewish respondents

---

## Warm Up 1

### Regress age at first birth (`agekdbrn`) on years of education (`educ`)

::: {.fragment}

```{r}
#| echo: true
#| output: false

warmup_1 <- lm(agekdbrn ~ educ, data = gss_week11)

summary(warmup_1)
```

:::

---

## Warm Up 1

```{r}
#| echo: false

summary(warmup_1)
```

---

## Warm Up 1

### Predict age at first birth for respondents with 16 years of education

::: {.fragment}

```{r}
#| echo: true
#| output-location: fragment

13.46735 + .79237*16
```

:::

---

## Warm Up 2

### Regress age at first birth (`agekdbrn`) on highest degree (`degree`). Use "College Degree" as the reference group.

::: {.fragment}

```{r}
#| echo: true
#| output: false

gss_week11 <- mutate(gss_week11, degree = factor(degree, 
      levels = c("Less Than HS", "HS Diploma",
      "Some College", "College Degree",
      "Grad/Prof Degree")))

gss_week11$degree <- relevel(factor(gss_week11$degree), 
  ref = "College Degree")

warmup2 <- lm(agekdbrn ~ degree, data = gss_week11)

summary(warmup2)
```

:::

---

## Warm Up 2

```{r}
#| echo: false

summary(warmup2)
```

---

## Warm Up 2

### Predict age at first birth for respondents with a graduate or professional degree

::: {.fragment}

```{r}
#| echo: true
#| output-location: fragment

27.3598 + .3829
```

:::

---

## Warm Up 3

### Regress having a first child at age 30 or later (`agekdbrn_30plus`) on religion (`religion`). Use "Protestant" as the reference group.

::: {.fragment}

```{r}
#| echo: true
#| output: false

gss_week11$religion <- relevel(factor(gss_week11$religion), 
                               ref="Protestant") 

warmup3 <- lm(agekdbrn_30plus ~ religion, data = gss_week11)

summary(warmup3)
```

:::

---

## Warm Up 3

```{r}
#| echo: false

summary(warmup3)
```

---

## Warm Up 3

### Predict probability of having a first child at age 30 or later for Jewish respondents:

::: {.fragment}

```{r}
#| echo: true
#| output-location: fragment

.149915 + .331567
```

:::

---

## Introducing Multiple Regression

- So far, our models have had one X (even if it has more than one category)

- We want to adjust for possible confounding or spuriousness like we did with descriptive tables
  - How do we *control for* other variables?
  - Can we *explain away* the association between X and Y by controlling for other variables?

---

## Introducing Multiple Regression

- Find another variable, *hold it constant*, and see if the association between X and Y changes

- Can be categorical (Highest Degree, Year, Religion) or continuous (Years Since Marriage, Months Since Sister's First Birth)

---

## Introducing Multiple Regression

- We already saw that each additional year of education is associated with a delay of .79 years in the age at first birth.

- Perhaps religion explains some of the variation in both education and age at first birth. So let's *hold religion constant*.

- In R, include more variables by linking them to the independent variable with a plus sign

::: {.fragment}

```{r}
#| echo: true
#| output: false

agekd_educ_religion <- lm(agekdbrn ~ educ + religion, 
  data = gss_week11)

summary(agekd_educ_religion)
```

:::

---

## Introducing Multiple Regression

```{r}
#| echo: false

summary(agekd_educ_religion)
```

---

## Introducing Multiple Regression

- Holding religion constant (or net of religion), each additional year of education is associated with a delay of .76 years in the age at first birth, on average

- To find the predicted values, think of the full equation:

::: {.fragment}

$\hat{y}_{agekdbrn} = \alpha + \beta_1 (educ) + \beta_2 (Catholic) + \beta_3 (Eastern) +$ 
$\beta_4 (Jewish) + \beta_5 (None) + \beta_6 (Other)$


:::

- Every prediction will have a value for education. Every prediction will also have a value for each binary religious category (even though they are mutually exclusive).

---

## Predictions From Multiple Regression

- For 16 years of education and Protestant (the reference category)

::: {.fragment}

```{r}
#| echo: true
#| output-location: fragment

13.32 + .76*16 + 1.55*0 + 3.45*0 + 2.85*0 - .14*0 + 1.25*0
```

:::

- Try finding the predicted age at first birth for Catholic respondents with 16 years of education

::: {.fragment}

```{r}
#| echo: true
#| output-location: fragment

13.32 + .76*16 + 1.55*1 + 3.45*0 + 2.85*0 - .14*0 + 1.25*0
```

:::

- Is the difference of 1.55 in the predictions between Protestants and Catholics *with the same years of education* statistically significant?

---

## Predictions From Multiple Regression

```{r}
#| echo: false

summary(agekd_educ_religion)
```

---

## Predictions From Multiple Regression

- What is the prediction for a respondent in an Eastern religion with 13 years of education?

::: {.fragment}

```{r}
#| echo: true
#| output-location: fragment

13.32 + .76*13 + 1.55*0 + 3.45*1 +2.85*0 - .14*0 + 1.25*0
```

:::

---

## Plotting Multiple Regression

- How do we make sense of this in a plot?

- The beta for all groups is the coefficient for `educ`. So in this model the slopes are the same for each group.

- But the intercepts are different: use the intercept coefficient for the reference group, use the intercept and the respective coefficient for each other group

---

## Plotting Multiple Regression

```{r}
#| echo: false

agekd_educ_religion_plot <- ggplot(gss_week11, aes(x = educ, y = agekdbrn))
agekd_educ_religion_plot + geom_point(color = "Light Gray") +
     geom_abline(slope = .76529, intercept = 13.32194, color = "Blue") +
     geom_abline(slope = .76529, intercept = 13.32194 + 1.54599, color = "Forest Green") +
     geom_abline(slope = .76529, intercept = 13.32194 + 3.44548, color = "Red") + 
     geom_abline(slope = .76529, intercept = 13.32194 + 2.85010, color = "Purple") +
     geom_text(x = 8, y = 48, label = "Eastern", color = "Red", size = 10) +
     geom_text(x = 8, y = 45, label = "Jewish", color = "Purple", size = 10) +
     geom_text(x = 8, y = 42, label = "Catholic", color = "Forest Green", size = 10) +
     geom_text(x = 8, y = 39, label = "Protestant", color = "Blue", size = 10) +
     geom_point(x = 16, y = 25.48, color = "Blue", size = 2) +
     geom_point(x = 16, y = 27.03, color = "Forest Green", size = 2) +
     geom_point(x = 13, y = 26.65, color = "Red", size = 2) +
     labs(x = "Years of Education", y = "Age at First Birth", title = "Education and Age at First Birth", subtitle = "GSS, 2010-2016") + 
     theme(axis.title = element_text(size = 18), axis.text = element_text(size = 16),
      plot.title = element_text(size = 18), plot.subtitle = element_text(size = 14))
```

---

## More And More Variables

- Models can continue adding control variables

- Let's try regressing age at first birth on education, religion, and race

::: {.fragment}

```{r}
#| echo: true
#| output: false

agekd_educ_religion_race_model <- 
lm(agekdbrn ~ educ + religion + racehisp,
data = gss_week11)

summary(agekd_educ_religion_race_model)
```

:::

---

## More And More Variables


```{r}
#| echo: false

summary(agekd_educ_religion_race_model)
```




---

## More And More Variables

- Holding religion and race constant, each additional year of education is associated with a delay of .74 years in age at first birth, on average

- Controlling for education and race, Catholic women are 1.5 years older than Protestant women at their first birth, on average. This difference is significant.

- Net of education and religion, there is no significant difference in the age at first birth between Black women and women in the other race category, on average. 

- Holding constant, controlling for, and net of can all be used interchangeably in these examples.

---

## More And More Variables

- Predictions still require the full equation

- What is the predicted age at first birth for a Black Protestant with 17 years of education?

- Black is the reference group for `racehisp` and Protestant is the reference group for `religion` so:

::: {.fragment}

```{r}
#| echo: true
#| output-location: fragment

12.8134 + .7420*17
```

:::

---

## More And More Variables

- What is the predicted age at first birth for a Hispanic with no religious affiliation with 14 years of education?

::: {.fragment}

```{r}
#| echo: true
#| output-location: fragment

12.8134 + .7420*14 - .1464 + .2763
```

:::

---

## Comparing Models

- How do we know if our model gets better when we add more control variables?

- In other words: how well does our X predict our Y?

- Without an X, only comparison is the difference between the observed Y and the mean of Y

- With an X, the measure of fit is the residual (the difference between the observed Y and the predicted Y)

- $r^2$ is a function of both of these in the form of a ratio
  - The proportional reduction in error from using the model

---

## R-Squared

![](385_figures/rsquared_1.png)

---


## R-Squared

- Let's calculate $r^2$ for the model regressing number of memberships on years of education

- $\Large{r^2 = \frac{\sum{(y-\bar{y})^2} - \sum{(y-\hat{y})^2}} {\sum{(y-\bar{y})^2}}}$

::: {.fragment}

```{r}
#| echo: true
#| output: false

memnum_educ_model <- 
lm(memnum ~ educ, data = gss_week11)

summary(memnum_educ_model)
```

:::

---

## R-Squared

![](385_figures/rsquared_1.png)

---

## R-Squared

::: {.fragment}

```{r}
#| echo: true
#| output-location: fragment

memnum_educ_model <- lm(memnum ~ educ, data = gss_week11)
```

:::

::: {.fragment}

```{r}
#| echo: true
#| output-location: fragment

gss_week11$pred_memnum <- memnum_educ_model$fitted.values

gss_week11$res_memnum <- 
(gss_week11$memnum - gss_week11$pred_memnum)^2

gss_week11$dev_memnum <- 
(gss_week11$memnum - mean(gss_week11$memnum))^2

rsquared <- ((sum(gss_week11$dev_memnum)) - 
                  (sum(gss_week11$res_memnum))) /
                  sum(gss_week11$dev_memnum)

rsquared
```

:::

---

## Properties of R-Squared

- Like correlation, always between 0 and 1

- Unlike correlation, always positive (since it is squared and a proportion)

- Closer to 1 means observations fall more tightly around the line (in a linear association)

- Will usually increase when you add variables to the model. But that does not necessarily mean the model is getting better.

- Remember, parsimony is still our goal

---

## Comparing Models

- If we regress number of memberships on education and age, it looks like the model is better since r-squared increases.

![](385_figures/rsquared_3.png)

---

## Comparing Models

- But be careful: R-squared will almost always go up as you add variables, even if the variables are not significant. 

![](385_figures/rsquared_4.png)

---

## Adjusted R Squared

- Adjusted r-squared adjusts for the number of parameters in your model (but not for how good they are)

![](385_figures/rsquared_2.png)

---

## Adjusted R-Squared

::: {.fragment}

```{r}
#| echo: true
#| output-location: fragment

# adjusted_rsquared = 
# 1 - (((1 - rsquared)*(n-1)) / (n-k-1))

# n = sample size; k = number of variables

adjusted_rsquared <- 
1 - (((1 - rsquared)*(1924-1)) / (1924-1-1))

adjusted_rsquared
```

:::

---

## Adjusted R-Squared

![](385_figures/rsquared_2.png)
